import json
import re
import time
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pymongo import MongoClient
from aiTaskRunner import AITaskRunner
from config import (
    OUTPUT_BASE_DIR, CHUNKS_DIR, MERGED_DIR, RESULT_DIR,
    MONGODB_CONFIG, TASK_CONFIG, PROMPT_FILES,
    DEFAULT_EXECUTION_CONFIG, get_api_config, get_optimal_chunk_params,
    get_speculative_config, _ensure_directories_exist
)
from config import SERVICE_MAPPING

# æœåŠ¡æ˜ å°„é…ç½®
SERVICE_MAPPING["find_content_index"] = "doubao"
SERVICE_MAPPING["generate_questions"] = "deepseek"  # åˆ‡æ¢åˆ°ç¨³å®šçš„æœ¬åœ°æœåŠ¡
SERVICE_MAPPING["extract_rules"] = "doubao"

# å¤šæ¨¡å‹é…ç½® - ç”¨äºç»´åº¦åˆ†é…
MULTI_MODEL_CONFIG = {
    "enabled": True,  # æ˜¯å¦å¯ç”¨å¤šæ¨¡å‹åˆ†é…
    "available_services": ["deepseek", "doubao"],  # å¯ç”¨æœåŠ¡åˆ—è¡¨ï¼ˆå‡å°‘kimiä½¿ç”¨ï¼‰
    "allocation_strategy": "round_robin",  # åˆ†é…ç­–ç•¥ï¼šround_robin(è½®è¯¢), random(éšæœº)
    "exclude_unstable": True  # æ˜¯å¦æ’é™¤ä¸ç¨³å®šçš„æœåŠ¡
}

# ç»´åº¦å¤§å°åˆ†ç±»é…ç½®
DIMENSION_SIZE_CONFIG = {
    "enabled": True,  # æ˜¯å¦å¯ç”¨æŒ‰å¤§å°åˆ†ç±»å¤„ç†
    "small_threshold": 6000,  # å°ç»´åº¦é˜ˆå€¼ï¼ˆå­—ç¬¦æ•°ï¼‰
    "large_threshold": 9000,  # å¤§ç»´åº¦é˜ˆå€¼ï¼ˆå­—ç¬¦æ•°ï¼‰
    "chunk_large_dimensions": True,  # æ˜¯å¦å¯¹å¤§ç»´åº¦è¿›è¡Œåˆ†å—
    "chunk_size": 6000,  # å¤§ç»´åº¦åˆ†å—å¤§å°
    "chunk_overlap": 1000,  # åˆ†å—é‡å å¤§å°
    "small_batch_size": 5,  # å°ç»´åº¦æ‰¹å¤„ç†å¤§å°
    "small_processing": {
        "preferred_services": ["deepseek", "doubao"],  # å°ç»´åº¦ä¼˜å…ˆæœåŠ¡ï¼ˆå‡å°‘kimiä½¿ç”¨ï¼‰
        "max_tokens": 6000  # å°ç»´åº¦tokenæ•°
    },
    "medium_processing": {
        "preferred_services": ["deepseek", "doubao"],  # ä¸­ç»´åº¦æœåŠ¡ï¼ˆç§»é™¤kimiï¼‰
        "max_tokens": 8000  # ä¸­ç»´åº¦tokenæ•°
    },
    "large_processing": {
        "strategy": "chunk_and_merge",  # å¤§ç»´åº¦å¤„ç†ç­–ç•¥
        "use_powerful_models": True,  # å¤§ç»´åº¦ä½¿ç”¨æ›´å¼ºæ¨¡å‹
        "preferred_services": ["deepseek"],  # å¤§ç»´åº¦ä¼˜å…ˆæœåŠ¡
        "max_tokens": 8000  # å¤§ç»´åº¦æœ€å¤§tokenæ•°
    }
}

# ============================ åˆå§‹åŒ–è®¾ç½® ============================
_ensure_directories_exist()

# ============================ å·¥å…·å‡½æ•° ============================

def log_speculative_status() -> None:
    """è®°å½• Speculative Decoding çŠ¶æ€"""
    print("[Speculative Decoding] å·²å¼ºåˆ¶ç¦ç”¨ - ä½¿ç”¨æ ‡å‡†è°ƒç”¨æ¨¡å¼")

def log_saved_file(file_path: Path) -> None:
    """å°†ä¿å­˜çš„æ–‡ä»¶è·¯å¾„è¿½åŠ å†™å…¥æ—¥å¿—æ–‡ä»¶"""
    try:
        log_file = OUTPUT_BASE_DIR / "saved_files.log"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | {file_path}\n")
        print(f"[è®°å½•] å·²å†™å…¥æ—¥å¿—: {file_path}")
    except Exception as e:
        print(f"[è­¦å‘Š] å†™å…¥æ—¥å¿—å¤±è´¥: {e}")

def allocate_service_for_dimensions(dimensions: List[Dict[str, Any]]) -> List[str]:
    """
    ä¸ºæ¯ä¸ªç»´åº¦åˆ†é…æœåŠ¡
    Args:
        dimensions: ç»´åº¦åˆ—è¡¨
    Returns:
        List[str]: æ¯ä¸ªç»´åº¦å¯¹åº”çš„æœåŠ¡åç§°åˆ—è¡¨
    """
    if not MULTI_MODEL_CONFIG["enabled"]:
        # å¦‚æœæœªå¯ç”¨å¤šæ¨¡å‹ï¼Œæ‰€æœ‰ç»´åº¦ä½¿ç”¨é»˜è®¤æœåŠ¡
        default_service = SERVICE_MAPPING.get("generate_questions", "doubao")
        return [default_service] * len(dimensions)

    available_services = MULTI_MODEL_CONFIG["available_services"].copy()
    strategy = MULTI_MODEL_CONFIG["allocation_strategy"]

    # è¿‡æ»¤æ‰ä¸ç¨³å®šçš„æœåŠ¡ï¼ˆå¦‚æœå¯ç”¨äº†æ’é™¤é€‰é¡¹ï¼‰
    if MULTI_MODEL_CONFIG.get("exclude_unstable", False):
        # å¯ä»¥æ ¹æ®éœ€è¦å®šä¹‰ä¸ç¨³å®šçš„æœåŠ¡
        unstable_services = ["qwen-plus"]  # ä¹‹å‰é‡åˆ°è¶…æ—¶çš„æœåŠ¡
        available_services = [s for s in available_services if s not in unstable_services]

    allocated_services = []

    if strategy == "round_robin":
        # è½®è¯¢åˆ†é…
        for i, dimension in enumerate(dimensions):
            service = available_services[i % len(available_services)]
            allocated_services.append(service)
            print(f"[å¤šæ¨¡å‹åˆ†é…] ç»´åº¦ '{dimension['name']}' -> æœåŠ¡ '{service}'")

    elif strategy == "random":
        # éšæœºåˆ†é…
        import random
        for dimension in dimensions:
            service = random.choice(available_services)
            allocated_services.append(service)
            print(f"[å¤šæ¨¡å‹åˆ†é…] ç»´åº¦ '{dimension['name']}' -> æœåŠ¡ '{service}'")

    else:
        # é»˜è®¤è½®è¯¢
        for i, dimension in enumerate(dimensions):
            service = available_services[i % len(available_services)]
            allocated_services.append(service)

    print(f"[å¤šæ¨¡å‹åˆ†é…] å…±åˆ†é… {len(dimensions)} ä¸ªç»´åº¦åˆ° {len(set(allocated_services))} ä¸ªä¸åŒæœåŠ¡")
    return allocated_services

def analyze_dimension_sizes(dimensions: List[Dict[str, Any]], chunks: List[Dict]) -> Dict[str, Any]:
    """
    åˆ†æç»´åº¦å¤§å°å¹¶åˆ†ç±»
    Args:
        dimensions: ç»´åº¦åˆ—è¡¨
        chunks: æ–‡æ¡£åˆ†å—
    Returns:
        Dict: åŒ…å«åˆ†ç±»ç»“æœçš„å­—å…¸
    """
    if not DIMENSION_SIZE_CONFIG["enabled"]:
        return {
            "small_dimensions": dimensions,
            "medium_dimensions": [],
            "large_dimensions": [],
            "analysis": {}
        }

    small_threshold = DIMENSION_SIZE_CONFIG["small_threshold"]
    large_threshold = DIMENSION_SIZE_CONFIG["large_threshold"]

    small_dimensions = []
    medium_dimensions = []
    large_dimensions = []
    dimension_sizes = []

    print(f"[ç»´åº¦åˆ†æ] å¼€å§‹åˆ†æ {len(dimensions)} ä¸ªç»´åº¦çš„å¤§å°")
    print(f"[ç»´åº¦åˆ†æ] é˜ˆå€¼è®¾ç½®: å°ç»´åº¦ < {small_threshold}, å¤§ç»´åº¦ > {large_threshold}")

    for dimension in dimensions:
        # è®¡ç®—ç»´åº¦å†…å®¹å¤§å°
        content = extract_dimension_content(chunks, dimension, dimensions)
        content_size = len(content)
        dimension_sizes.append(content_size)

        # æ·»åŠ å¤§å°ä¿¡æ¯åˆ°ç»´åº¦
        dimension["content_size"] = content_size
        dimension["content_preview"] = content[:200] + "..." if len(content) > 200 else content

        # åˆ†ç±»
        if content_size <= small_threshold:
            small_dimensions.append(dimension)
            category = "å°ç»´åº¦"
        elif content_size >= large_threshold:
            large_dimensions.append(dimension)
            category = "å¤§ç»´åº¦"
        else:
            medium_dimensions.append(dimension)
            category = "ä¸­ç»´åº¦"

        print(f"[ç»´åº¦åˆ†æ] '{dimension['name']}': {content_size} å­—ç¬¦ ({category})")

    # ç»Ÿè®¡åˆ†æ
    analysis = {
        "total_count": len(dimensions),
        "small_count": len(small_dimensions),
        "medium_count": len(medium_dimensions),
        "large_count": len(large_dimensions),
        "sizes": dimension_sizes,
        "avg_size": sum(dimension_sizes) / len(dimension_sizes) if dimension_sizes else 0,
        "max_size": max(dimension_sizes) if dimension_sizes else 0,
        "min_size": min(dimension_sizes) if dimension_sizes else 0
    }

    print(f"[ç»´åº¦åˆ†æ] åˆ†ç±»ç»“æœ: å°ç»´åº¦ {analysis['small_count']} ä¸ª, ä¸­ç»´åº¦ {analysis['medium_count']} ä¸ª, å¤§ç»´åº¦ {analysis['large_count']} ä¸ª")
    print(f"[ç»´åº¦åˆ†æ] å¤§å°ç»Ÿè®¡: å¹³å‡ {analysis['avg_size']:.0f} å­—ç¬¦, æœ€å¤§ {analysis['max_size']} å­—ç¬¦, æœ€å° {analysis['min_size']} å­—ç¬¦")

    return {
        "small_dimensions": small_dimensions,
        "medium_dimensions": medium_dimensions,
        "large_dimensions": large_dimensions,
        "analysis": analysis
    }

def find_question_boundaries(content: str) -> List[int]:
    """
    æ‰¾åˆ°é¢˜ç›®è¾¹ç•Œä½ç½®
    Returns:
        List[int]: é¢˜ç›®å¼€å§‹ä½ç½®çš„ç´¢å¼•åˆ—è¡¨
    """
    import re
    boundaries = [0]  # å¼€å§‹ä½ç½®

    # é¢˜ç›®ç¼–å·æ¨¡å¼
    question_patterns = [
        r'\n\s*(\d+)\s*[ï¼.ã€]\s*',      # 1. 1ï¼ 1ã€
        r'\n\s*\((\d+)\)\s*',           # (1) (2)
        r'\n\s*ï¼ˆ(\d+)ï¼‰\s*',           # ï¼ˆ1ï¼‰ï¼ˆ2ï¼‰
        r'\n\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[ï¼.ã€]\s*',  # ä¸€ã€äºŒã€
        r'\n\s*\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s*',      # (ä¸€)(äºŒ)
        r'\n\s*ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰\s*',      # ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰
    ]

    for pattern in question_patterns:
        matches = re.finditer(pattern, content)
        for match in matches:
            pos = match.start() + 1  # è·³è¿‡æ¢è¡Œç¬¦
            if pos not in boundaries:
                boundaries.append(pos)

    # æ’åºå¹¶å»é‡
    boundaries = sorted(set(boundaries))
    boundaries.append(len(content))  # æ·»åŠ ç»“æŸä½ç½®

    return boundaries

def chunk_large_dimension(dimension: Dict[str, Any], content: str) -> List[Dict[str, Any]]:
    """
    è¯­ä¹‰æ„ŸçŸ¥çš„å¤§ç»´åº¦åˆ†å—å¤„ç†
    Args:
        dimension: ç»´åº¦ä¿¡æ¯
        content: ç»´åº¦å†…å®¹
    Returns:
        List[Dict]: åˆ†å—åçš„å­ç»´åº¦åˆ—è¡¨
    """
    if not DIMENSION_SIZE_CONFIG["chunk_large_dimensions"]:
        return [dimension]

    chunk_size = DIMENSION_SIZE_CONFIG["chunk_size"]
    chunk_overlap = DIMENSION_SIZE_CONFIG["chunk_overlap"]

    if len(content) <= chunk_size:
        return [dimension]

    print(f"[æ™ºèƒ½åˆ†å—] å¼€å§‹åˆ†å—ç»´åº¦ '{dimension['name']}', å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")

    # 1. æ‰¾åˆ°æ‰€æœ‰é¢˜ç›®è¾¹ç•Œ
    question_boundaries = find_question_boundaries(content)
    print(f"[æ™ºèƒ½åˆ†å—] æ£€æµ‹åˆ° {len(question_boundaries)-1} ä¸ªé¢˜ç›®è¾¹ç•Œ")

    # 2. è¯­ä¹‰æ„ŸçŸ¥åˆ†å—
    chunks = []
    chunk_index = 1
    current_start = 0

    while current_start < len(content):
        # å¯»æ‰¾æœ€ä½³åˆ†å—ç‚¹
        optimal_end = current_start + chunk_size

        if optimal_end >= len(content):
            # æœ€åä¸€å—
            chunk_content = content[current_start:]
            chunk_dimension = create_chunk_dimension(dimension, chunk_content, chunk_index, current_start, len(content))
            chunks.append(chunk_dimension)
            break

        # åœ¨chunk_sizeèŒƒå›´å†…æ‰¾åˆ°æœ€ä½³çš„é¢˜ç›®è¾¹ç•Œ
        best_boundary = optimal_end
        for boundary in question_boundaries:
            if current_start < boundary <= optimal_end + 1000:  # å…è®¸é€‚å½“è¶…å‡º
                best_boundary = boundary
            elif boundary > optimal_end + 1000:
                break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„è¾¹ç•Œï¼Œä½¿ç”¨æ™ºèƒ½åˆ‡åˆ†
        if best_boundary == optimal_end:
            best_boundary = find_safe_split_point(content, current_start, optimal_end)

        # åˆ›å»ºåˆ†å—
        chunk_content = content[current_start:best_boundary]

        # è´¨é‡æ£€æŸ¥
        if validate_chunk_quality(chunk_content):
            chunk_dimension = create_chunk_dimension(dimension, chunk_content, chunk_index, current_start, best_boundary)
            chunks.append(chunk_dimension)

            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ç‚¹ï¼ˆè€ƒè™‘é‡å ï¼‰
            if best_boundary < len(content):
                overlap_start = max(current_start, best_boundary - chunk_overlap)
                # å¯»æ‰¾é‡å åŒºåŸŸå†…çš„åˆé€‚èµ·å§‹ç‚¹
                next_start = find_overlap_start_point(content, overlap_start, best_boundary)
                current_start = next_start
            else:
                current_start = best_boundary

            chunk_index += 1
        else:
            print(f"[æ™ºèƒ½åˆ†å—] åˆ†å—è´¨é‡ä¸ä½³ï¼Œè°ƒæ•´åˆ†å—ç‚¹")
            # å¦‚æœè´¨é‡ä¸ä½³ï¼Œç¼©å°åˆ†å—
            best_boundary = find_safe_split_point(content, current_start, current_start + chunk_size // 2)
            chunk_content = content[current_start:best_boundary]
            chunk_dimension = create_chunk_dimension(dimension, chunk_content, chunk_index, current_start, best_boundary)
            chunks.append(chunk_dimension)
            current_start = best_boundary
            chunk_index += 1

    print(f"[æ™ºèƒ½åˆ†å—] ç»´åº¦ '{dimension['name']}' åˆ†æˆ {len(chunks)} ä¸ªè¯­ä¹‰å®Œæ•´çš„åˆ†å—")

    # åˆ†å—è´¨é‡æŠ¥å‘Š
    for i, chunk in enumerate(chunks):
        content_preview = chunk["chunk_content"][:100].replace('\n', ' ')
        print(f"[åˆ†å—{i+1}] {chunk['content_size']} å­—ç¬¦: {content_preview}...")

    return chunks

def create_chunk_dimension(dimension: Dict[str, Any], chunk_content: str, chunk_index: int, start_pos: int, end_pos: int) -> Dict[str, Any]:
    """åˆ›å»ºåˆ†å—ç»´åº¦å¯¹è±¡"""
    chunk_dimension = dimension.copy()
    chunk_dimension["name"] = f"{dimension['name']}_åˆ†å—{chunk_index}"
    chunk_dimension["original_name"] = dimension["name"]
    chunk_dimension["is_chunk"] = True
    chunk_dimension["chunk_index"] = chunk_index
    chunk_dimension["content_size"] = len(chunk_content)
    chunk_dimension["chunk_content"] = chunk_content
    chunk_dimension["start_position"] = start_pos
    chunk_dimension["end_position"] = end_pos
    return chunk_dimension

def find_safe_split_point(content: str, start: int, target_end: int) -> int:
    """æ‰¾åˆ°å®‰å…¨çš„åˆ†å—ç‚¹ï¼Œé¿å…ç ´åè¯­ä¹‰ç»“æ„"""
    # ä¼˜å…ˆçº§ï¼šé¢˜ç›®è¾¹ç•Œ > æ®µè½è¾¹ç•Œ > å¥å­è¾¹ç•Œ > æ ‡ç‚¹ç¬¦å·

    # 1. åœ¨ç›®æ ‡èŒƒå›´å†…å¯»æ‰¾é¢˜ç›®å¼€å§‹
    question_pattern = r'(\n\s*[\dï¼ˆ(][\d)ï¼‰]?\s*[ï¼.ã€)]?\s*)'
    import re
    matches = list(re.finditer(question_pattern, content[start:target_end]))
    if matches:
        last_match = matches[-1]
        return start + last_match.start() + 1

    # 2. å¯»æ‰¾æ®µè½è¾¹ç•Œï¼ˆè¿ç»­æ¢è¡Œï¼‰
    paragraph_pattern = r'\n\s*\n'
    matches = list(re.finditer(paragraph_pattern, content[start:target_end]))
    if matches:
        last_match = matches[-1]
        return start + last_match.end()

    # 3. å¯»æ‰¾å¥å­è¾¹ç•Œ
    sentence_ends = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
    for i in range(target_end - 1, start, -1):
        if content[i] in sentence_ends and i + 1 < len(content) and content[i + 1] in ['\n', ' ']:
            return i + 1

    # 4. å¯»æ‰¾å…¶ä»–æ ‡ç‚¹ç¬¦å·
    punctuation = ['ï¼›', ';', 'ï¼Œ', ',', 'ï¼š', ':']
    for i in range(target_end - 1, start, -1):
        if content[i] in punctuation:
            return i + 1

    # 5. æœ€åå¯»æ‰¾ç©ºæ ¼
    for i in range(target_end - 1, start, -1):
        if content[i] == ' ':
            return i + 1

    return target_end

def find_overlap_start_point(content: str, overlap_start: int, chunk_end: int) -> int:
    """åœ¨é‡å åŒºåŸŸå†…æ‰¾åˆ°åˆé€‚çš„èµ·å§‹ç‚¹"""
    # å¯»æ‰¾é¢˜ç›®å¼€å§‹ä½œä¸ºæ–°çš„èµ·å§‹ç‚¹
    import re
    question_pattern = r'(\n\s*[\dï¼ˆ(][\d)ï¼‰]?\s*[ï¼.ã€)]?\s*)'
    matches = list(re.finditer(question_pattern, content[overlap_start:chunk_end]))
    if matches:
        # é€‰æ‹©æœ€åä¸€ä¸ªé¢˜ç›®å¼€å§‹
        last_match = matches[-1]
        return overlap_start + last_match.start() + 1

    return overlap_start

def validate_chunk_quality(chunk_content: str) -> bool:
    """éªŒè¯åˆ†å—è´¨é‡"""
    if len(chunk_content.strip()) < 100:  # å¤ªçŸ­
        return False

    # æ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„é¢˜ç›®
    import re
    question_pattern = r'[\dï¼ˆ(][\d)ï¼‰]?\s*[ï¼.ã€)]?\s*.{10,}'
    if not re.search(question_pattern, chunk_content):
        return False

    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„æˆªæ–­ï¼ˆå¦‚æœ«å°¾æ²¡æœ‰æ ‡ç‚¹ç¬¦å·ï¼‰
    last_char = chunk_content.strip()[-1] if chunk_content.strip() else ''
    if last_char.isalnum():  # ä»¥å­—æ¯æˆ–æ•°å­—ç»“å°¾ï¼Œå¯èƒ½è¢«æˆªæ–­
        return False

    return True

def process_large_dimensions_separately(
    large_dimensions: List[Dict[str, Any]],
    chunks: List[Dict],
    **kwargs
) -> List[Dict[str, Any]]:
    """
    å•ç‹¬å¤„ç†å¤§ç»´åº¦
    """
    if not large_dimensions:
        return []

    print(f"[å¤§ç»´åº¦å¤„ç†] å¼€å§‹å¤„ç† {len(large_dimensions)} ä¸ªå¤§ç»´åº¦")

    # å¯¹å¤§ç»´åº¦è¿›è¡Œåˆ†å—
    all_chunked_dimensions = []
    for dimension in large_dimensions:
        content = dimension.get("content_preview", "") or extract_dimension_content(chunks, dimension, large_dimensions)
        chunked_dims = chunk_large_dimension(dimension, content)
        all_chunked_dimensions.extend(chunked_dims)

    print(f"[å¤§ç»´åº¦å¤„ç†] åˆ†å—åå…± {len(all_chunked_dimensions)} ä¸ªå¤„ç†å•å…ƒ")

    # ä¸ºå¤§ç»´åº¦åˆ†é…æ›´å¼ºçš„æœåŠ¡
    if DIMENSION_SIZE_CONFIG["large_processing"]["use_powerful_models"]:
        preferred_services = DIMENSION_SIZE_CONFIG["large_processing"]["preferred_services"]
        print(f"[å¤§ç»´åº¦å¤„ç†] ä½¿ç”¨ä¼˜é€‰æœåŠ¡: {preferred_services}")

        # é‡æ–°é…ç½®æœåŠ¡åˆ†é…
        original_services = MULTI_MODEL_CONFIG["available_services"].copy()
        MULTI_MODEL_CONFIG["available_services"] = preferred_services

        allocated_services = allocate_service_for_dimensions(all_chunked_dimensions)

        # æ¢å¤åŸå§‹é…ç½®
        MULTI_MODEL_CONFIG["available_services"] = original_services
    else:
        allocated_services = allocate_service_for_dimensions(all_chunked_dimensions)

    # ä½¿ç”¨æ›´é«˜çš„tokené™åˆ¶
    max_tokens = DIMENSION_SIZE_CONFIG["large_processing"]["max_tokens"]

    # å¹¶è¡Œå¤„ç†åˆ†å—
    results = []
    from concurrent.futures import ThreadPoolExecutor, as_completed

    def process_single_large_dimension(dim_and_service):
        dimension, service_name = dim_and_service
        return generate_questions_by_dimension(
            dimension=dimension,
            chunks=chunks,
            all_dimensions=all_chunked_dimensions,
            service_name=service_name,
            max_tokens=max_tokens,
            **kwargs
        )

    dimension_service_pairs = list(zip(all_chunked_dimensions, allocated_services))

    with ThreadPoolExecutor(max_workers=min(len(all_chunked_dimensions), 10)) as executor:
        futures = [executor.submit(process_single_large_dimension, pair) for pair in dimension_service_pairs]
        for future in as_completed(futures):
            result = future.result()
            results.append(result)
            dim_name = result.get("dimension", "æœªçŸ¥")
            success = result.get("success", False)
            question_count = len(result.get("questions", []))
            print(f"[å¤§ç»´åº¦å¤„ç†] åˆ†å— '{dim_name}' å®Œæˆ: {'æˆåŠŸ' if success else 'å¤±è´¥'}, é¢˜ç›®æ•°: {question_count}")

    return results

def load_chunks_json(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½åˆ†å—JSONæ–‡ä»¶"""
    file_path_obj = Path(file_path)
    if not file_path_obj.exists():
        print(f"[é”™è¯¯] chunks æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []

    try:
        with open(file_path_obj, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, list) else []
    except Exception as e:
        print(f"[é”™è¯¯] è¯»å– JSON å¤±è´¥: {e}")
        return []

def create_safe_filename(title: str) -> str:
    """åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å"""
    return "".join(c for c in title if c.isalnum() or c in " _-")

def extract_content_by_index_range(chunks: List[Dict], start_idx: int, end_idx: int) -> str:
    """æ ¹æ®ç´¢å¼•èŒƒå›´æå–å†…å®¹ï¼ˆä¸è¿›è¡Œè¾¹ç•Œæ¸…ç†ï¼‰"""
    content_list = []
    for idx in range(start_idx, end_idx + 1):
        list_idx = idx - 1  # è½¬æ¢ä¸ºåˆ—è¡¨ç´¢å¼•
        if 0 <= list_idx < len(chunks):
            content = chunks[list_idx].get("content", "")
            if content.strip():
                content_list.append(content)
        else:
            print(f"[è­¦å‘Š] ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ (chunksæ€»æ•°={len(chunks)})")

    merged_content = "\n".join(content_list)
    print(f"[å†…å®¹æå–] ç´¢å¼•èŒƒå›´ {start_idx}-{end_idx}ï¼Œæå–å†…å®¹é•¿åº¦: {len(merged_content)} å­—ç¬¦")
    return merged_content

def extract_dimension_content(chunks: List[Dict], dimension: Dict[str, Any], all_dimensions: List[Dict[str, Any]]) -> str:
    """
    ä¸ºæŒ‡å®šç»´åº¦æå–å†…å®¹å¹¶è¿›è¡Œç²¾ç¡®çš„è¾¹ç•Œæ¸…ç†
    Args:
        chunks: æ–‡æ¡£åˆ†å—
        dimension: å½“å‰ç»´åº¦ä¿¡æ¯
        all_dimensions: æ‰€æœ‰ç»´åº¦åˆ—è¡¨ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªæˆ–æœ€åä¸€ä¸ªï¼‰
    """
    start_idx = dimension["start_index"]
    end_idx = dimension["end_index"]

    # è·å–åŸå§‹å†…å®¹
    raw_content = extract_content_by_index_range(chunks, start_idx, end_idx)

    # åˆ¤æ–­ç»´åº¦ä½ç½®
    is_first = dimension == all_dimensions[0] if all_dimensions else False
    is_last = dimension == all_dimensions[-1] if all_dimensions else False

    # åº”ç”¨ç»´åº¦çº§åˆ«çš„è¾¹ç•Œæ¸…ç†
    cleaned_content = clean_dimension_boundary(
        content=raw_content,
        dimension_info=dimension,
        is_first=is_first,
        is_last=is_last
    )

    return cleaned_content

def clean_dimension_boundary(content: str, dimension_info: Dict[str, Any], is_first: bool = False, is_last: bool = False) -> str:
    """
    æ ¹æ®ç»´åº¦ä¿¡æ¯ç²¾ç¡®æ¸…ç†å†…å®¹è¾¹ç•Œ
    Args:
        content: åŸå§‹å†…å®¹
        dimension_info: ç»´åº¦ä¿¡æ¯ï¼ŒåŒ…å«nameç­‰å­—æ®µ
        is_first: æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªç»´åº¦
        is_last: æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªç»´åº¦
    """
    if not is_first and not is_last:
        # ä¸­é—´ç»´åº¦ä¿æŒå®Œæ•´ï¼Œä¸åšè¾¹ç•Œæ¸…ç†
        print(f"[ç»´åº¦è¾¹ç•Œ] ä¸­é—´ç»´åº¦ '{dimension_info.get('name', '')}' ä¿æŒå®Œæ•´å†…å®¹")
        return content

    lines = content.split('\n')
    dimension_name = dimension_info.get('name', '')

    # å®šä¹‰è¯„ä¼°æ ‡é¢˜çš„æ¨¡å¼
    evaluation_patterns = [
        r'å…¨å¸‚æ€§.*?è¯„ä¼°æŒ‡æ ‡$',
        r'.*?è¯„ä¼°æ ‡å‡†$',
        r'.*?è¯„ä¼°åŠæ³•$',
        r'.*?è¯„ä¼°ä½“ç³»$',
        r'.*?ç®¡ç†è¯„ä¼°.*?$',
        r'æ°‘åŠéä¼ä¸šå•ä½.*?è¯„ä¼°.*?æŒ‡æ ‡$',
        r'åŸºé‡‘ä¼š.*?è¯„ä¼°.*?æŒ‡æ ‡$',
        r'ç¤¾ä¼šå›¢ä½“.*?è¯„ä¼°.*?æŒ‡æ ‡$',
        r'å¼‚åœ°å•†ä¼š.*?è¯„ä¼°.*?æŒ‡æ ‡$',
        r'è¡Œä¸šåä¼š.*?è¯„ä¼°.*?æŒ‡æ ‡$',
        r'åŒºåŸŸæ€§.*?è¯„ä¼°.*?$',
        r'.*?å•†ä¼š.*?è¯„ä¼°.*?$'
    ]

    cleaned_lines = []
    start_collecting = not is_first  # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªç»´åº¦ï¼Œä»å¼€å§‹å°±æ”¶é›†

    for i, line in enumerate(lines):
        line_stripped = line.strip()

        # ç¬¬ä¸€ä¸ªç»´åº¦ï¼šæŸ¥æ‰¾ç»´åº¦æ ‡é¢˜ï¼Œä»æ ‡é¢˜å¼€å§‹æ”¶é›†
        if is_first and not start_collecting:
            # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°äº†å½“å‰ç»´åº¦çš„æ ‡é¢˜
            if dimension_name in line_stripped:
                print(f"[ç¬¬ä¸€ç»´åº¦] æ‰¾åˆ°ç»´åº¦æ ‡é¢˜: {line_stripped}")
                start_collecting = True
                cleaned_lines.append(line)
                continue
            else:
                # è·³è¿‡æ ‡é¢˜å‰çš„å†…å®¹
                continue

        # æœ€åä¸€ä¸ªç»´åº¦ï¼šæ£€æµ‹ä¸‹ä¸€ä¸ªè¯„ä¼°æ ‡é¢˜ï¼Œåœæ­¢æ”¶é›†
        if is_last:
            found_next_evaluation = False
            for pattern in evaluation_patterns:
                if re.search(pattern, line_stripped):
                    # éªŒè¯è¿™ç¡®å®æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ ‡é¢˜
                    if (len(line_stripped) < 25 and
                        'è¯„ä¼°' in line_stripped and
                        ('æŒ‡æ ‡' in line_stripped or 'æ ‡å‡†' in line_stripped or 'åŠæ³•' in line_stripped or 'ä½“ç³»' in line_stripped)):
                        print(f"[æœ€åç»´åº¦] å‘ç°ä¸‹ä¸€ä¸ªè¯„ä¼°æ ‡é¢˜ï¼Œåœæ­¢æ”¶é›†: {line_stripped}")
                        found_next_evaluation = True
                        break

            if found_next_evaluation:
                break

        # æ”¶é›†å†…å®¹
        if start_collecting:
            cleaned_lines.append(line)

    cleaned_content = '\n'.join(cleaned_lines)

    # ç»Ÿè®¡ä¿¡æ¯
    original_len = len(content)
    cleaned_len = len(cleaned_content)
    removed_len = original_len - cleaned_len

    if is_first and removed_len > 0:
        print(f"[ç¬¬ä¸€ç»´åº¦æ¸…ç†] ç§»é™¤æ ‡é¢˜å‰å†…å®¹: {removed_len} å­—ç¬¦")
    elif is_last and removed_len > 0:
        print(f"[æœ€åç»´åº¦æ¸…ç†] ç§»é™¤ä¸‹ä¸€è¯„ä¼°æ ‡é¢˜åå†…å®¹: {removed_len} å­—ç¬¦")

    print(f"[ç»´åº¦è¾¹ç•Œ] '{dimension_name}' æ¸…ç†å®Œæˆ: {original_len} -> {cleaned_len} å­—ç¬¦")
    return cleaned_content

def clean_content_boundary(content: str) -> str:
    """
    ä¿æŒå‘åå…¼å®¹çš„è¾¹ç•Œæ¸…ç†å‡½æ•°ï¼ˆå·²å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨clean_dimension_boundaryï¼‰
    """
    print("[è­¦å‘Š] clean_content_boundaryå·²å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨clean_dimension_boundary")
    return content

# ============================ æ•°æ®åº“æ“ä½œ ============================

def extract_chunks_to_file(
    mongo_host: str = MONGODB_CONFIG.get("host", "localhost"),
    mongo_port: int = MONGODB_CONFIG.get("port", 27017),
    mongo_user: Optional[str] = MONGODB_CONFIG.get("username"),
    mongo_password: Optional[str] = MONGODB_CONFIG.get("password"),
    db_name: str = MONGODB_CONFIG.get("db_name", "doc_db"),
    collection_name: str = MONGODB_CONFIG.get("collection_name", "doc_batches"),
    output_dir: Path = CHUNKS_DIR,
    target_batch_id: Optional[int] = None
) -> Optional[Dict[str, Any]]:
    """ä»MongoDBæå–åˆ†å—æ•°æ®å¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
    try:
        # æ„å»ºè¿æ¥
        if mongo_user and mongo_password:
            client = MongoClient(
                host=mongo_host,
                port=mongo_port,
                username=mongo_user,
                password=mongo_password,
                authSource=MONGODB_CONFIG.get("auth_source", "admin"),
                authMechanism=MONGODB_CONFIG.get("auth_mechanism", "SCRAM-SHA-256"),
                serverSelectionTimeoutMS=MONGODB_CONFIG.get("server_selection_timeout_ms", 5000)
            )
        else:
            client = MongoClient(
                host=mongo_host,
                port=mongo_port,
                serverSelectionTimeoutMS=MONGODB_CONFIG.get("server_selection_timeout_ms", 5000)
            )

        client.admin.command("ping")
        collection = client[db_name][collection_name]

        # è·å–ç›®æ ‡ batch
        if target_batch_id is not None:
            last_record = collection.find_one({"batch_id": target_batch_id})
            if not last_record:
                print(f"[è­¦å‘Š] æœªæ‰¾åˆ° batch_id={target_batch_id} çš„è®°å½•")
                client.close()
                return None
        else:
            last_record = collection.find_one(sort=[("batch_id", -1)])
            if not last_record:
                print(f"[æç¤º] æ•°æ®åº“ {collection_name} ä¸­æ²¡æœ‰åˆ†å—æ•°æ®")
                client.close()
                return None

        batch_id = last_record["batch_id"]
        chunks = last_record.get("chunks", [])
        client.close()

        if not chunks:
            print(f"[æç¤º] batch_id={batch_id} ä¸‹æœªæå–åˆ°ä»»ä½•åˆ†å—æ•°æ®")
            return None

        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_subdir = output_dir / f"extract_{timestamp}"
        output_subdir.mkdir(parents=True, exist_ok=True)
        json_file = output_subdir / f"chunks_batch{batch_id}_{timestamp}.json"

        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)

        print(f"[å®Œæˆ] å·²ä¿å­˜ batch_id={batch_id} çš„åˆ†å—æ•°æ®: {json_file} (å…± {len(chunks)} ä¸ª)")
        log_saved_file(json_file)
        return {"batch_id": batch_id, "chunks": chunks, "chunks_file": str(json_file)}

    except Exception as e:
        print(f"[é”™è¯¯] MongoDB è¿æ¥æˆ–æŸ¥è¯¢å¤±è´¥: {e}")
        return None

# ============================ å¢å¼ºçš„ç´¢å¼•æå–åŠŸèƒ½ ============================

def find_content_index_with_dimensions(
    title: str,
    chunks_file: str,
    api_key: str = None,
    prompt_file: str = PROMPT_FILES["index"],
    output_dir: Path = MERGED_DIR,
    base_url: str = None,
    model: str = None,
    retry: int = TASK_CONFIG["retry_count"]
) -> Optional[Dict[str, Any]]:
    """
    æŸ¥æ‰¾æ ‡é¢˜å¯¹åº”çš„å†…å®¹ç´¢å¼•å¹¶è¯†åˆ«æ‰€æœ‰ç»´åº¦
    è¿”å›å¢å¼ºçš„ç»“æœï¼ŒåŒ…å«ç»´åº¦ä¿¡æ¯
    """
    chunks = load_chunks_json(chunks_file)
    if not chunks:
        print("[é”™è¯¯] åŠ è½½ chunks å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return None

    # è·å– API é…ç½®
    if not api_key or not base_url or not model:
        config = get_api_config("find_content_index")
        api_key = api_key or config["api_key"]
        base_url = base_url or config["base_url"]
        model = model or config["model"]
        print(f"[è°ƒè¯•] find_content_index_with_dimensions ä½¿ç”¨é…ç½®: {base_url} | {model}")

    try:
        with open(prompt_file, "r", encoding="utf-8") as f:
            base_prompt_template = f.read().strip()
    except Exception as e:
        print(f"[é”™è¯¯] è¯»å–æç¤ºè¯æ–‡ä»¶å¤±è´¥: {e}")
        return None

    # å¢å¼ºæç¤ºè¯ï¼Œè¦æ±‚åŒæ—¶è¯†åˆ«ç»´åº¦
    enhanced_prompt_template = base_prompt_template + """

**é¢å¤–è¦æ±‚ï¼šç»´åº¦è¯†åˆ«**
åœ¨æ‰¾åˆ°ç›®æ ‡å†…å®¹åï¼Œè¯·åŒæ—¶åˆ†ææ•´ä¸ªå†…å®¹èŒƒå›´ï¼Œè¯†åˆ«å…¶ä¸­åŒ…å«çš„æ‰€æœ‰è¯„ä¼°ç»´åº¦ã€‚

è¯·åœ¨è¿”å›çš„JSONä¸­å¢åŠ ä»¥ä¸‹å­—æ®µï¼š
```json
{
  // ... åŸæœ‰å­—æ®µ ...
  "dimensions": [
    {
      "name": "ç»´åº¦åç§°",
      "start_index": ç»´åº¦èµ·å§‹ç´¢å¼•,
      "end_index": ç»´åº¦ç»“æŸç´¢å¼•,
      "description": "ç»´åº¦æè¿°"
    }
  ]
}
```

**ç»´åº¦è¯†åˆ«è§„åˆ™ï¼š**
1. ç»´åº¦é€šå¸¸æ˜¯æœ€å¤§çš„åˆ†ç±»å±‚çº§ï¼ˆå¦‚"ä¸€ã€äºŒã€ä¸‰ã€"æˆ–"ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ï¼ˆä¸‰ï¼‰"ï¼‰
2. æ¯ä¸ªç»´åº¦åŒ…å«ä¸€ç»„ç›¸å…³çš„è¯„ä¼°é¢˜ç›®
3. ç»´åº¦è¾¹ç•Œä»¥ä¸‹ä¸€ä¸ªåŒçº§ç»´åº¦æ ‡é¢˜ä¸ºå‡†
4. ç¡®ä¿ç»´åº¦ç´¢å¼•èŒƒå›´ä¸é‡å ä¸”å®Œæ•´è¦†ç›–å†…å®¹èŒƒå›´
"""

    doc_text = "\n".join([
        f"Index {c.get('chunk_index', idx+1)}:\n{c.get('content', '')}"
        for idx, c in enumerate(chunks)
    ])
    prompt = enhanced_prompt_template.replace("{target_title}", title).replace("{doc_text}", doc_text)

    # è·å– Speculative Decoding é…ç½®
    spec_config = get_speculative_config("find_content_index")
    runner = AITaskRunner(
        api_key=api_key,
        base_url=base_url,
        model=model,
        enable_speculative=False,  # å¼ºåˆ¶ç¦ç”¨Speculative
        speculative_times=1
    )
    output_dir.mkdir(parents=True, exist_ok=True)

    for attempt in range(retry):
        try:
            res = runner.run_task(prompt, response_format="json")
            print(f"[AI å“åº” - å°è¯• {attempt+1}]: {res}")

            if not isinstance(res, dict):
                continue
            if not res.get("found", False):
                print(f"[è­¦å‘Š] AI æœªæ‰¾åˆ°æ ‡é¢˜: {title}")
                continue

            start_idx = res.get("start_index")
            end_idx = res.get("end_index")
            dimensions = res.get("dimensions", [])

            print(f"[è°ƒè¯•] AIè¿”å›ç´¢å¼•èŒƒå›´: start_idx={start_idx}, end_idx={end_idx}")
            print(f"[è°ƒè¯•] AIè¯†åˆ«çš„ç»´åº¦æ•°é‡: {len(dimensions)}")

            if start_idx is None or end_idx is None:
                print("[è­¦å‘Š] æœªè·å–åˆ°æœ‰æ•ˆçš„ç´¢å¼•èŒƒå›´")
                continue

            # éªŒè¯ç»´åº¦ä¿¡æ¯
            valid_dimensions = []
            for dim in dimensions:
                if (dim.get("start_index") is not None and
                    dim.get("end_index") is not None and
                    dim.get("name")):
                    valid_dimensions.append(dim)
                    print(f"[ç»´åº¦] {dim['name']}: ç´¢å¼• {dim['start_index']}-{dim['end_index']}")

            if not valid_dimensions:
                print("[è­¦å‘Š] æœªè¯†åˆ«åˆ°æœ‰æ•ˆç»´åº¦ï¼Œå°†ä½¿ç”¨æ•´ä½“å†…å®¹ä½œä¸ºå•ä¸€ç»´åº¦")
                valid_dimensions = [{
                    "name": title,
                    "start_index": start_idx,
                    "end_index": end_idx,
                    "description": "æ•´ä½“å†…å®¹"
                }]

            # æå–å¹¶ä¿å­˜åˆå¹¶å†…å®¹
            merged_content = extract_content_by_index_range(chunks, start_idx, end_idx)
            safe_title = create_safe_filename(title)
            merged_file = output_dir / f"{safe_title}_merged.json"

            with open(merged_file, "w", encoding="utf-8") as f:
                json.dump({
                    "title": title,
                    "content": merged_content,
                    "dimensions": valid_dimensions
                }, f, ensure_ascii=False, indent=2)
            log_saved_file(merged_file)

            print(f"[å®Œæˆ] å†…å®¹ç´¢å¼•å’Œç»´åº¦è¯†åˆ«å®Œæˆ: {merged_file}")

            return {
                "title": title,
                "start_index": start_idx,
                "end_index": end_idx,
                "dimensions": valid_dimensions,
                "merged_content": merged_content,
                "merged_content_file": str(merged_file),
                "chunks": chunks,
                "total_count": res.get("total_count", len(range(start_idx, end_idx + 1))),
                "stop_reason": res.get("stop_reason", ""),
                "content_summary": res.get("content_summary", "")
            }

        except Exception as e:
            print(f"[è­¦å‘Š] AI è°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{retry}): {e}")
            continue

    print("[å¤±è´¥] æ‰€æœ‰é‡è¯•å‡æœªæˆåŠŸ")
    return None

# ============================ æŒ‰ç»´åº¦ç”Ÿæˆé¢˜ç›® ============================

def generate_questions_by_dimension(
    dimension: Dict[str, Any],
    chunks: List[Dict],
    all_dimensions: List[Dict[str, Any]] = None,
    api_key: str = None,
    prompt_file: str = PROMPT_FILES["question"],
    base_url: str = None,
    model: str = None,
    max_tokens: int = TASK_CONFIG["max_tokens"],
    retry: int = TASK_CONFIG["retry_count"],
    service_name: str = None  # æ–°å¢ï¼šæŒ‡å®šä½¿ç”¨çš„æœåŠ¡
) -> Dict[str, Any]:
    """
    ä¸ºå•ä¸ªç»´åº¦ç”Ÿæˆç»“æ„åŒ–é¢˜ç›®
    """
    dimension_name = dimension["name"]
    start_idx = dimension["start_index"]
    end_idx = dimension["end_index"]

    print(f"[ç»´åº¦å¤„ç†] å¼€å§‹å¤„ç†ç»´åº¦: {dimension_name} (ç´¢å¼• {start_idx}-{end_idx})")

    # è·å– API é…ç½®ï¼ˆæ”¯æŒæŒ‡å®šæœåŠ¡ï¼‰
    if not api_key or not base_url or not model:
        if service_name:
            # ä½¿ç”¨æŒ‡å®šçš„æœåŠ¡è·å–é…ç½®
            from config import API_SERVICES, API_KEYS
            if service_name in API_SERVICES and service_name in API_KEYS:
                config = API_SERVICES[service_name].copy()
                config["api_key"] = API_KEYS[service_name]
                print(f"[ç»´åº¦ {dimension_name}] ä½¿ç”¨æŒ‡å®šæœåŠ¡: {service_name}")
            else:
                print(f"[è­¦å‘Š] æœåŠ¡ {service_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                config = get_api_config("generate_questions")
        else:
            config = get_api_config("generate_questions")

        api_key = api_key or config["api_key"]
        base_url = base_url or config["base_url"]
        model = model or config["model"]
        print(f"[ç»´åº¦ {dimension_name}] ä½¿ç”¨é…ç½®: {base_url} | {model}")

    # è¯»å–æç¤ºè¯æ¨¡æ¿
    try:
        with open(prompt_file, "r", encoding="utf-8") as f:
            template = f.read().strip()
    except Exception as e:
        return {
            "dimension": dimension_name,
            "questions": [],
            "error": f"æ— æ³•è¯»å–æç¤ºè¯: {e}"
        }

    # æå–ç»´åº¦å†…å®¹ï¼ˆä½¿ç”¨æ–°çš„è¾¹ç•Œæ¸…ç†é€»è¾‘ï¼‰
    dimension_content = extract_dimension_content(chunks, dimension, all_dimensions or [])
    if not dimension_content.strip():
        print(f"[è­¦å‘Š] ç»´åº¦ {dimension_name} å†…å®¹ä¸ºç©º")
        return {
            "dimension": dimension_name,
            "questions": [],
            "error": "ç»´åº¦å†…å®¹ä¸ºç©º"
        }

    print(f"[ç»´åº¦å¤„ç†] ç»´åº¦ {dimension_name} å†…å®¹é•¿åº¦: {len(dimension_content)} å­—ç¬¦")

    # å¢å¼ºæç¤ºè¯ï¼Œæ·»åŠ ç»´åº¦ä¸Šä¸‹æ–‡
    enhanced_template = template + f"""

**å½“å‰å¤„ç†ç»´åº¦**: {dimension_name}
**ç»´åº¦è¯´æ˜**: {dimension.get('description', 'æ— ')}

è¯·ç¡®ä¿æå–çš„é¢˜ç›®éƒ½å±äº"{dimension_name}"è¿™ä¸ªç»´åº¦ï¼Œé¢˜å·åº”åœ¨è¯¥ç»´åº¦å†…ä¿æŒè¿ç»­æ€§ã€‚
"""

    prompt = enhanced_template.replace("{doc_content}", dimension_content)

    # è·å– Speculative Decoding é…ç½®
    spec_config = get_speculative_config("generate_questions")
    runner = AITaskRunner(
        api_key=api_key,
        base_url=base_url,
        model=model,
        max_tokens=max_tokens,
        enable_speculative=False,  # å¼ºåˆ¶ç¦ç”¨Speculative
        speculative_times=1
    )

    for attempt in range(retry):
        try:
            res = runner.run_task(prompt, response_format="json")
            print(f"[ç»´åº¦ {dimension_name}] AIå“åº” (å°è¯• {attempt+1}): {type(res)}")
            if isinstance(res, str):
                print(f"[è°ƒè¯•] å“åº”å†…å®¹é¢„è§ˆ: {res[:200]}...")

            if isinstance(res, str):
                cleaned = re.sub(r"```(?:json)?\n(.*?)```", r"\1", res, flags=re.S).strip()
                cleaned = cleaned.replace("<result>", "").replace("</result>", "").strip()
                try:
                    result_data = json.loads(cleaned)
                except json.JSONDecodeError as e:
                    print(f"[ç»´åº¦ {dimension_name}] JSONè§£æå¤±è´¥: {e}")
                    print(f"[è°ƒè¯•] åŸå§‹å“åº”ç±»å‹: {type(res)}")
                    print(f"[è°ƒè¯•] æ¸…ç†åå†…å®¹: {cleaned[:500]}...")
                    continue
            else:
                result_data = res

            if not isinstance(result_data, dict):
                continue

            # å°è¯•å¤šç§å¯èƒ½çš„å­—æ®µå
            question_data = (result_data.get("question_data", []) or
                           result_data.get("questions", []) or
                           result_data.get("data", []) or
                           result_data.get("items", []) or
                           result_data.get("results", []))

            if not question_data:
                print(f"[ç»´åº¦ {dimension_name}] æœªæå–åˆ°é¢˜ç›®æ•°æ®")
                print(f"[è°ƒè¯•] AIè¿”å›çš„å­—æ®µ: {list(result_data.keys())}")
                print(f"[è°ƒè¯•] AIè¿”å›å†…å®¹é¢„è§ˆ: {str(result_data)[:500]}...")
                continue

            # éªŒè¯å¹¶æ•´ç†é¢˜ç›®æ•°æ®ï¼Œç¡®ä¿æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡º
            valid_questions = []
            total_questions = 0

            # å¤„ç†é¢˜ç›®æ•°æ®ï¼Œä¸ç®¡åŸå§‹ç»“æ„å¦‚ä½•ï¼Œéƒ½æ•´ç†æˆç»Ÿä¸€æ ¼å¼
            for dim_group in question_data:
                if isinstance(dim_group, dict):
                    # å¦‚æœæœ‰questionså­—æ®µï¼Œæå–å…¶ä¸­çš„é¢˜ç›®
                    if "questions" in dim_group:
                        questions = dim_group["questions"]
                        for q in questions:
                            if (q.get("sort_number") is not None and
                                q.get("standard_score") is not None and
                                q.get("stem")):
                                valid_questions.append(q)
                                total_questions += 1
                    # å¦‚æœç›´æ¥å°±æ˜¯é¢˜ç›®å¯¹è±¡
                    elif (dim_group.get("sort_number") is not None and
                          dim_group.get("standard_score") is not None and
                          dim_group.get("stem")):
                        valid_questions.append(dim_group)
                        total_questions += 1

            # æŒ‰é¢˜å·æ’åºï¼Œç¡®ä¿è¿ç»­æ€§
            valid_questions.sort(key=lambda x: x.get("sort_number", 0))

            print(f"[ç»´åº¦ {dimension_name}] æˆåŠŸæå– {total_questions} ä¸ªæœ‰æ•ˆé¢˜ç›®")

            # è¿”å›ç¬¦åˆç”¨æˆ·è¦æ±‚çš„æ ¼å¼ï¼šdimension + questionsæ•°ç»„
            return {
                "dimension": dimension_name,
                "questions": valid_questions,
                "success": True
            }

        except Exception as e:
            print(f"[ç»´åº¦ {dimension_name}] å¤„ç†å¤±è´¥ (å°è¯• {attempt+1}/{retry}): {e}")
            continue

    return {
        "dimension": dimension_name,
        "questions": [],
        "error": f"æ‰€æœ‰é‡è¯•å‡å¤±è´¥",
        "success": False
    }

def generate_questions_by_dimensions_parallel(
    dimensions: List[Dict[str, Any]],
    chunks: List[Dict],
    api_key: str = None,
    prompt_file: str = PROMPT_FILES["question"],
    output_dir: Path = RESULT_DIR,
    base_url: str = None,
    model: str = None,
    max_tokens: int = TASK_CONFIG["max_tokens"],
    retry: int = TASK_CONFIG["retry_count"],
    max_workers: int = TASK_CONFIG["max_workers"],
    renumber_questions: bool = True  # æ–°å¢ï¼šæ˜¯å¦é‡æ–°ç¼–å·é¢˜ç›®
) -> Dict[str, Any]:
    """
    å¹¶è¡Œå¤„ç†æ‰€æœ‰ç»´åº¦ï¼Œç”Ÿæˆç»“æ„åŒ–é¢˜ç›®
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"[å¹¶è¡Œå¤„ç†] å¼€å§‹å¹¶è¡Œå¤„ç† {len(dimensions)} ä¸ªç»´åº¦")

    # åˆ†æç»´åº¦å¤§å°å¹¶åˆ†ç±»
    dimension_analysis = analyze_dimension_sizes(dimensions, chunks)
    small_dimensions = dimension_analysis["small_dimensions"]
    medium_dimensions = dimension_analysis["medium_dimensions"]
    large_dimensions = dimension_analysis["large_dimensions"]

    print(f"[ç»´åº¦åˆ†ç±»] å°ç»´åº¦: {len(small_dimensions)} ä¸ª, ä¸­ç»´åº¦: {len(medium_dimensions)} ä¸ª, å¤§ç»´åº¦: {len(large_dimensions)} ä¸ª")

    # ğŸš€ å…¨æ–°å¹¶è¡Œç­–ç•¥ï¼šæ‰€æœ‰å¤„ç†å•å…ƒåŒæ—¶å¹¶è¡Œæ‰§è¡Œ
    print(f"\n[è¶…çº§å¹¶è¡Œ] å‡†å¤‡æ‰€æœ‰å¤„ç†å•å…ƒ...")

    all_processing_units = []  # æ‰€æœ‰éœ€è¦å¤„ç†çš„å•å…ƒ

    # 1. å‡†å¤‡å¤§ç»´åº¦åˆ†å—å•å…ƒ
    if large_dimensions:
        print(f"[è¶…çº§å¹¶è¡Œ] å‡†å¤‡å¤§ç»´åº¦åˆ†å—...")
        for dimension in large_dimensions:
            content = extract_dimension_content(chunks, dimension, large_dimensions)
            chunked_dims = chunk_large_dimension(dimension, content)
            for chunk_dim in chunked_dims:
                # ä¸ºå¤§ç»´åº¦åˆ†å—åˆ†é…å¼ºåŠ›æœåŠ¡
                preferred_services = DIMENSION_SIZE_CONFIG["large_processing"]["preferred_services"]
                service = preferred_services[len(all_processing_units) % len(preferred_services)]
                all_processing_units.append({
                    "dimension": chunk_dim,
                    "service": service,
                    "max_tokens": DIMENSION_SIZE_CONFIG["large_processing"]["max_tokens"],
                    "type": "large_chunk"
                })

    # 2. å‡†å¤‡å°ç»´åº¦å•å…ƒ
    if small_dimensions:
        print(f"[è¶…çº§å¹¶è¡Œ] å‡†å¤‡å°ç»´åº¦...")
        preferred_services = DIMENSION_SIZE_CONFIG["small_processing"]["preferred_services"]
        for i, dimension in enumerate(small_dimensions):
            service = preferred_services[i % len(preferred_services)]
            all_processing_units.append({
                "dimension": dimension,
                "service": service,
                "max_tokens": DIMENSION_SIZE_CONFIG["small_processing"]["max_tokens"],
                "type": "small"
            })

    # 3. å‡†å¤‡ä¸­ç»´åº¦å•å…ƒ
    if medium_dimensions:
        print(f"[è¶…çº§å¹¶è¡Œ] å‡†å¤‡ä¸­ç»´åº¦...")
        preferred_services = DIMENSION_SIZE_CONFIG["medium_processing"]["preferred_services"]
        for i, dimension in enumerate(medium_dimensions):
            service = preferred_services[i % len(preferred_services)]
            all_processing_units.append({
                "dimension": dimension,
                "service": service,
                "max_tokens": DIMENSION_SIZE_CONFIG["medium_processing"]["max_tokens"],
                "type": "medium"
            })

    print(f"[è¶…çº§å¹¶è¡Œ] æ€»å…±å‡†å¤‡äº† {len(all_processing_units)} ä¸ªå¤„ç†å•å…ƒï¼Œå³å°†åŒæ—¶å¹¶è¡Œæ‰§è¡Œï¼")

    # ç»Ÿè®¡æœåŠ¡åˆ†å¸ƒ
    service_stats = {}
    for unit in all_processing_units:
        service_stats[unit["service"]] = service_stats.get(unit["service"], 0) + 1
    print(f"[è¶…çº§å¹¶è¡Œ] æœåŠ¡åˆ†å¸ƒ: {service_stats}")

    def process_any_unit(processing_unit):
        """é€šç”¨å¤„ç†å•å…ƒå‡½æ•°"""
        dimension = processing_unit["dimension"]
        service_name = processing_unit["service"]
        max_tokens = processing_unit["max_tokens"]
        unit_type = processing_unit["type"]

        return generate_questions_by_dimension(
            dimension=dimension,
            chunks=chunks,
            all_dimensions=dimensions,  # ä¼ é€’æ‰€æœ‰ç»´åº¦ç”¨äºä¸Šä¸‹æ–‡
            api_key=api_key,
            prompt_file=prompt_file,
            base_url=base_url,
            model=model,
            max_tokens=max_tokens,
            retry=retry,
            service_name=service_name
        )

    # ğŸš€ è¶…çº§å¹¶è¡Œæ‰§è¡Œï¼šæ‰€æœ‰å•å…ƒåŒæ—¶å¤„ç†
    print(f"\n[è¶…çº§å¹¶è¡Œ] å¯åŠ¨ {len(all_processing_units)} ä¸ªå¹¶è¡Œä»»åŠ¡...")
    all_results = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [executor.submit(process_any_unit, unit) for unit in all_processing_units]

        # å®æ—¶æ”¶é›†ç»“æœ
        completed_count = 0
        for future in as_completed(futures):
            result = future.result()
            all_results.append(result)
            completed_count += 1

            # å®æ—¶è¿›åº¦æŠ¥å‘Š
            dim_name = result.get("dimension", "æœªçŸ¥")
            success = result.get("success", False)
            question_count = len(result.get("questions", []))

            # è¯†åˆ«å¤„ç†å•å…ƒç±»å‹
            unit_type = "æœªçŸ¥"
            if "_åˆ†å—" in dim_name:
                unit_type = "å¤§ç»´åº¦åˆ†å—"
            elif any(unit["dimension"]["name"] == dim_name for unit in all_processing_units if unit["type"] == "small"):
                unit_type = "å°ç»´åº¦"
            elif any(unit["dimension"]["name"] == dim_name for unit in all_processing_units if unit["type"] == "medium"):
                unit_type = "ä¸­ç»´åº¦"

            progress = f"({completed_count}/{len(all_processing_units)})"
            print(f"[è¶…çº§å¹¶è¡Œ] {progress} {unit_type} '{dim_name}' å®Œæˆ: {'âœ…æˆåŠŸ' if success else 'âŒå¤±è´¥'}, é¢˜ç›®æ•°: {question_count}")

    print(f"\n[è¶…çº§å¹¶è¡Œ] ğŸ‰ æ‰€æœ‰ {len(all_processing_units)} ä¸ªå¤„ç†å•å…ƒå·²å¹¶è¡Œå®Œæˆï¼")

    print(f"\n[å¤„ç†å®Œæˆ] æ€»å…±å¤„ç†äº† {len(all_results)} ä¸ªå¤„ç†å•å…ƒ")

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_questions_by_dimension = []
    total_questions = 0
    successful_dimensions = 0
    current_question_number = 1  # ä»ç¬¬1é¢˜å¼€å§‹è¿ç»­ç¼–å·

    # å¤„ç†ç»“æœï¼Œåˆå¹¶åˆ†å—çš„å¤§ç»´åº¦
    merged_results = {}
    for result in all_results:
        if result.get("success", False) and result.get("questions"):
            dimension_name = result["dimension"]

            # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†å—
            if "_åˆ†å—" in dimension_name:
                original_name = result.get("original_name") or dimension_name.split("_åˆ†å—")[0]
                if original_name not in merged_results:
                    merged_results[original_name] = {
                        "dimension": original_name,
                        "questions": [],
                        "chunks": []
                    }
                merged_results[original_name]["questions"].extend(result["questions"])
                merged_results[original_name]["chunks"].append(result)
                print(f"[åˆ†å—åˆå¹¶] åˆå¹¶åˆ†å— '{dimension_name}' åˆ° '{original_name}', é¢˜ç›®æ•°: {len(result['questions'])}")
            else:
                merged_results[dimension_name] = result

    # æŒ‰åŸå§‹ç»´åº¦é¡ºåºæ’åº
    original_dimension_names = [d["name"] for d in dimensions]
    sorted_merged_results = []

    for dim_name in original_dimension_names:
        if dim_name in merged_results:
            sorted_merged_results.append(merged_results[dim_name])
        else:
            # æŸ¥æ‰¾å¯èƒ½çš„åˆ†å—åˆå¹¶ç»“æœ
            for merged_name, merged_result in merged_results.items():
                if merged_name.startswith(dim_name) or dim_name.startswith(merged_name):
                    sorted_merged_results.append(merged_result)
                    break

    for result in sorted_merged_results:
        if result.get("success", False) and result.get("questions"):
            questions = result["questions"].copy()  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
            original_count = len(questions)

            # ç®€åŒ–å¤„ç†ï¼šåªæŒ‰é¢˜å·æ’åºï¼Œä¸è¿›è¡Œå¤æ‚çš„è¿‡æ»¤
            questions.sort(key=lambda x: x.get("sort_number", 0))
            print(f"[ç»´åº¦å¤„ç†] ç»´åº¦ '{result['dimension']}' åŒ…å« {len(questions)} ä¸ªé¢˜ç›®")

            if renumber_questions:
                # ä¸ºè¯¥ç»´åº¦çš„æ‰€æœ‰é¢˜ç›®é‡æ–°ç¼–å·
                for question in questions:
                    original_number = question.get("sort_number")
                    question["sort_number"] = current_question_number
                    question["original_sort_number"] = original_number  # ä¿å­˜åŸå§‹é¢˜å·
                    current_question_number += 1
                print(f"[é‡æ–°ç¼–å·] ç»´åº¦ '{result['dimension']}' çš„ {len(questions)} ä¸ªé¢˜ç›®å·²é‡æ–°ç¼–å·")
            else:
                # ä¿æŒåŸå§‹é¢˜å·ï¼ŒåªæŒ‰åŸå§‹é¢˜å·æ’åº
                questions.sort(key=lambda x: x.get("sort_number", 0))
                print(f"[ä¿æŒåŸå·] ç»´åº¦ '{result['dimension']}' çš„ {len(questions)} ä¸ªé¢˜ç›®ä¿æŒåŸå§‹é¢˜å·")

            dimension_questions = {
                "dimension": result["dimension"],
                "questions": questions
            }
            all_questions_by_dimension.append(dimension_questions)
            total_questions += len(questions)
            successful_dimensions += 1
        else:
            print(f"[è­¦å‘Š] ç»´åº¦ '{result.get('dimension')}' å¤„ç†å¤±è´¥æˆ–æ— é¢˜ç›®")

    print(f"[å¹¶è¡Œå¤„ç†] å®Œæˆç»Ÿè®¡: {successful_dimensions}/{len(dimensions)} ä¸ªç»´åº¦æˆåŠŸ, æ€»é¢˜ç›®æ•°: {total_questions}")

    # å¤šæ¨¡å‹åˆ†é…ç»Ÿè®¡ï¼ˆä½¿ç”¨ä¹‹å‰è®¡ç®—çš„service_statsï¼‰
    if MULTI_MODEL_CONFIG["enabled"]:
        print(f"[å¤šæ¨¡å‹ç»Ÿè®¡] æœ€ç»ˆæœåŠ¡ä½¿ç”¨åˆ†å¸ƒ: {service_stats}")

    if renumber_questions:
        print(f"[é‡æ–°ç¼–å·] é¢˜ç›®ç¼–å·å·²ä» 1 åˆ° {current_question_number - 1} è¿ç»­æ’åˆ—")
    else:
        print(f"[ä¿æŒåŸå·] é¢˜ç›®ä¿æŒåŸå§‹ç¼–å·ï¼ŒæŒ‰ç»´åº¦é¡ºåºæ’åˆ—")

    return {
        "success": True,
        "question_data": all_questions_by_dimension,
        "total_questions": total_questions
    }

# ============================ è§„åˆ™æå– ============================

def extract_rules_from_content(
    title: str,
    content_or_file: str,
    api_key: str = None,
    prompt_file: str = PROMPT_FILES["rule"],
    output_dir: Path = RESULT_DIR,
    base_url: str = None,
    model: str = None
) -> Dict[str, Any]:
    """ä»å†…å®¹ä¸­æå–è§„åˆ™"""
    # è·å– API é…ç½®
    if not api_key or not base_url or not model:
        config = get_api_config("extract_rules")
        api_key = api_key or config["api_key"]
        base_url = base_url or config["base_url"]
        model = model or config["model"]

    content = ""
    potential_path = Path(content_or_file)

    if potential_path.exists():
        try:
            with open(potential_path, "r", encoding="utf-8") as f:
                if potential_path.suffix.lower() == ".json":
                    data = json.load(f)
                    content = data.get("content", "").strip()
                else:
                    content = f.read().strip()
            if not content:
                return {"title": title, "error": "å†…å®¹ä¸ºç©º", "vetos_data": []}
        except Exception as e:
            return {"title": title, "error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}", "vetos_data": []}
    else:
        content = str(content_or_file).strip()

    if not content:
        return {"title": title, "error": "å†…å®¹ä¸ºç©º", "vetos_data": []}

    try:
        with open(prompt_file, "r", encoding="utf-8") as f:
            template = f.read()
    except Exception as e:
        return {"title": title, "error": f"æ— æ³•è¯»å–æç¤ºè¯: {e}", "vetos_data": []}

    prompt = template.replace("{text}", content)

    # è·å– Speculative Decoding é…ç½®
    spec_config = get_speculative_config("extract_rules")
    runner = AITaskRunner(
        api_key=api_key,
        base_url=base_url,
        model=model,
        enable_speculative=False,  # å¼ºåˆ¶ç¦ç”¨Speculative
        speculative_times=1
    )
    raw_response = runner.run_task(prompt, response_format="json")

    if isinstance(raw_response, str):
        cleaned = re.sub(r"```(?:json)?\n(.*?)```", r"\1", raw_response, flags=re.S).strip()
        cleaned = cleaned.replace("<result>", "").replace("</result>", "").strip()
        try:
            result = json.loads(cleaned)
        except Exception:
            result = {"raw_response": raw_response}
    else:
        result = raw_response

    vetos_data = result.get("vetos_data", []) if isinstance(result, dict) else []

    output_dir.mkdir(parents=True, exist_ok=True)
    safe_title = create_safe_filename(title)
    output_file = output_dir / f"{safe_title}_rules.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"[è®°å½•] ç”Ÿæˆçš„è§„åˆ™ JSON å·²ä¿å­˜: {output_file}")
    return {"title": title, "file": str(output_file), "result": result, "vetos_data": vetos_data}

# ============================ ä¸»æµç¨‹æ§åˆ¶ ============================

def process_pipeline_dimension_based(
    title: str = DEFAULT_EXECUTION_CONFIG["title"],
    api_key: str = None,
    zhipu_api_key: str = None,
    deepseek_api_key: str = None,
    index_prompt_file: str = PROMPT_FILES["index"],
    question_prompt_file: str = PROMPT_FILES["question"],
    rule_prompt_file: str = PROMPT_FILES["rule"],
    mongo_uri: str = None,
    db_name: str = MONGODB_CONFIG["db_name"],
    collection_name: str = MONGODB_CONFIG["collection_name"],
    target_batch_id: Optional[int] = DEFAULT_EXECUTION_CONFIG["target_batch_id"],
    renumber_questions: bool = True,  # æ–°å¢ï¼šæ˜¯å¦é‡æ–°ç¼–å·é¢˜ç›®ï¼ˆä»1å¼€å§‹è¿ç»­æ’åˆ—ï¼‰
    enable_content_filtering: bool = True,  # æ–°å¢ï¼šæ˜¯å¦å¯ç”¨å†…å®¹è¿‡æ»¤ï¼ˆæ£€æµ‹å¼‚å¸¸è·³è·ƒç­‰ï¼‰
    fast_mode: bool = True  # æ–°å¢ï¼šå¿«é€Ÿæ¨¡å¼ï¼Œä¼˜åŒ–æ€§èƒ½å‚æ•°
) -> Optional[Dict[str, Any]]:
    """
    æ‰§è¡ŒåŸºäºç»´åº¦åˆ†å—çš„å®Œæ•´ç”Ÿæˆæµç¨‹
    """
    print(f"\n{'='*60}\nå¼€å§‹å¤„ç†æ ‡é¢˜: {title} (åŸºäºç»´åº¦åˆ†å—)\n{'='*60}")

    # å¿«é€Ÿæ¨¡å¼é…ç½®
    if fast_mode:
        print("[å¿«é€Ÿæ¨¡å¼] å·²å¯ç”¨æ€§èƒ½ä¼˜åŒ–")
        print(f"[å¿«é€Ÿæ¨¡å¼] å¹¶è¡Œåº¦: {TASK_CONFIG['max_workers']} workers")
        print(f"[å¿«é€Ÿæ¨¡å¼] é‡è¯•æ¬¡æ•°: {TASK_CONFIG['retry_count']}")
        print(f"[å¿«é€Ÿæ¨¡å¼] æœ€å¤§token: {TASK_CONFIG['max_tokens']}")

    # æ˜¾ç¤º Speculative Decoding çŠ¶æ€
    log_speculative_status()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir_merged = MERGED_DIR / f"run_dimension_{timestamp}"
    run_dir_result = RESULT_DIR / f"run_dimension_{timestamp}"
    run_dir_merged.mkdir(parents=True, exist_ok=True)
    run_dir_result.mkdir(parents=True, exist_ok=True)

    # 1. æå– MongoDB åˆ†å—
    print("\n[1/5] æ­£åœ¨ä» MongoDB æå–åˆ†å—...")
    chunks_file_data = extract_chunks_to_file(target_batch_id=target_batch_id)
    if not chunks_file_data:
        return None
    chunks_file = chunks_file_data["chunks_file"]
    chunks = chunks_file_data["chunks"]
    print(f" æå–å®Œæˆ: {chunks_file}")

    # 2. æŸ¥æ‰¾æ ‡é¢˜ã€åˆå¹¶å†…å®¹å¹¶è¯†åˆ«ç»´åº¦
    print(f"\n[2/5] æ­£åœ¨æŸ¥æ‰¾æ ‡é¢˜ '{title}' å¹¶è¯†åˆ«ç»´åº¦...")
    index_result = find_content_index_with_dimensions(
        title=title,
        chunks_file=chunks_file,
        prompt_file=index_prompt_file,
        output_dir=run_dir_merged
    )
    if not index_result:
        return None

    dimensions = index_result.get("dimensions", [])
    merged_file_path = index_result.get("merged_content_file")

    print(f" è¯†åˆ«åˆ° {len(dimensions)} ä¸ªç»´åº¦")
    for dim in dimensions:
        print(f"   - {dim['name']}: ç´¢å¼• {dim['start_index']}-{dim['end_index']}")

    # 3&4. å¹¶å‘æ‰§è¡Œï¼šæå–è§„åˆ™ + æŒ‰ç»´åº¦ç”Ÿæˆç»“æ„åŒ–é¢˜ç›®
    print(f"\n[3&4/5] æ­£åœ¨å¹¶å‘æ‰§è¡Œï¼šæå–è§„åˆ™ + æŒ‰ç»´åº¦ç”Ÿæˆé¢˜ç›®...")

    def run_extract_rules():
        """å¹¶å‘æ‰§è¡Œï¼šæå–è§„åˆ™"""
        print(f"[å¹¶å‘ä»»åŠ¡1] å¼€å§‹æå–è§„åˆ™...")
        return extract_rules_from_content(
            title=title,
            content_or_file=merged_file_path,
            prompt_file=rule_prompt_file,
            output_dir=run_dir_result
        )

    def run_generate_questions():
        """å¹¶å‘æ‰§è¡Œï¼šæŒ‰ç»´åº¦ç”Ÿæˆç»“æ„åŒ–é¢˜ç›®"""
        print(f"[å¹¶å‘ä»»åŠ¡2] å¼€å§‹æŒ‰ç»´åº¦ç”Ÿæˆé¢˜ç›®...")
        print(f"[é…ç½®] é¢˜ç›®é‡æ–°ç¼–å·: {'å¯ç”¨' if renumber_questions else 'ç¦ç”¨'}")
        print(f"[é…ç½®] å†…å®¹è¿‡æ»¤: {'å¯ç”¨' if enable_content_filtering else 'ç¦ç”¨'}")
        return generate_questions_by_dimensions_parallel(
            dimensions=dimensions,
            chunks=chunks,
            prompt_file=question_prompt_file,
            output_dir=run_dir_result,
            renumber_questions=renumber_questions
        )

    # å¹¶å‘æ‰§è¡Œä¸¤ä¸ªä»»åŠ¡
    with ThreadPoolExecutor(max_workers=DEFAULT_EXECUTION_CONFIG["concurrent_tasks"]) as executor:
        rule_future = executor.submit(run_extract_rules)
        question_future = executor.submit(run_generate_questions)

        rule_result = rule_future.result()
        question_result = question_future.result()

        print(f"å¹¶å‘ä»»åŠ¡å®Œæˆï¼šè§„åˆ™æå–å’ŒæŒ‰ç»´åº¦é¢˜ç›®ç”Ÿæˆéƒ½å·²å®Œæˆ")

    # 5. ä¿å­˜æœ€ç»ˆç»“æœ - åªä¿ç•™ç”¨æˆ·éœ€è¦çš„ä¸¤ä¸ªå­—æ®µ
    unified_result = {
        "question_data": question_result.get("question_data", []),
        "vetos_data": rule_result.get("vetos_data", [])
    }

    # ä¿å­˜æœ€ç»ˆç»“æœæ–‡ä»¶
    safe_title = create_safe_filename(title)
    final_result_file = run_dir_result / f"{safe_title}_final.json"
    with open(final_result_file, "w", encoding="utf-8") as f:
        json.dump(unified_result, f, ensure_ascii=False, indent=2)
    log_saved_file(final_result_file)

    print(f"\n[å®Œæˆ] åŸºäºç»´åº¦åˆ†å—çš„æµç¨‹å®Œæˆï¼")
    print(f"å¤„ç†äº† {len(dimensions)} ä¸ªç»´åº¦ï¼Œç”Ÿæˆ {question_result.get('total_questions', 0)} ä¸ªé¢˜ç›®")
    print(f"ç»“æœæ–‡ä»¶: {final_result_file}")
    return unified_result

# ============================ ä¸»å‡½æ•° ============================

def main() -> None:
    """ä¸»å‡½æ•°"""
    start_time = time.time()

    print("="*80)
    print("åŸºäºç»´åº¦åˆ†å—çš„é¢˜ç›®ç”Ÿæˆç³»ç»Ÿ")
    print("="*80)

    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼ï¼Œå¦‚éœ€ä¿®æ”¹å¯åœ¨è¿™é‡Œè¦†ç›–
    result = process_pipeline_dimension_based()

    end_time = time.time()
    elapsed_seconds = end_time - start_time
    elapsed_str = f"{int(elapsed_seconds // 3600)}å°æ—¶ {int((elapsed_seconds % 3600) // 60)}åˆ†é’Ÿ {int(elapsed_seconds % 60)}ç§’"

    if result:
        question_count = len([q for dim in result.get("question_data", []) for q in dim.get("questions", [])])
        vetos_count = len(result.get("vetos_data", []))
        print(f"\n{'='*60}")
        print("å¤„ç†å®Œæˆ:")
        print(f"ç”Ÿæˆé¢˜ç›®æ•°: {question_count}")
        print(f"è§„åˆ™æ•°é‡: {vetos_count}")
        print(f"å¤„ç†æ—¶é—´: {elapsed_str}")
        print(f"{'='*60}")
    else:
        print("å¤„ç†å¤±è´¥")

    print(f"\nç¨‹åºæ€»è¿è¡Œæ—¶é—´: {elapsed_str}")

if __name__ == "__main__":
    main()